{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenización con Keras"
      ],
      "metadata": {
        "id": "-GjfB0XxghCJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWn1ph0mXLfy"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Clase Tokenizer de la biblioteca Keras para procesar un conjunto de frases y\n",
        "realizar varias operaciones relacionadas con la tokenización y la secuenciación de texto."
      ],
      "metadata": {
        "id": "RaHkLiRygvH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frases = [\n",
        "'Hola mundo',\n",
        "'Hola a todo',\n",
        "'Hola a todo el mundo']\n",
        "#Genera el diccionario de tokens\n",
        "tokenizer = Tokenizer(num_words = 10)\n",
        "tokenizer. fit_on_texts(frases)\n",
        "word_index = tokenizer.word_index\n",
        "print('\\nword_index =',word_index)\n",
        "#Generacion de secuencia tokenizadas\n",
        "secuencias = tokenizer. texts_to_sequences (frases)\n",
        "print('secuencias =',secuencias)\n",
        "#Rellena las secuencias a una longitud uniforme\n",
        "relleno = keras.preprocessing. sequence. pad_sequences (secuencias)\n",
        "print('rellena =\\n',relleno)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IJtWL30gdD1",
        "outputId": "998589f5-22ba-4aa7-f4cd-ede7ea5e356c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "word_index = {'hola': 1, 'mundo': 2, 'a': 3, 'todo': 4, 'el': 5}\n",
            "secuencias = [[1, 2], [1, 3, 4], [1, 3, 4, 5, 2]]\n",
            "rellena =\n",
            " [[0 0 0 1 2]\n",
            " [0 0 1 3 4]\n",
            " [1 3 4 5 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Número de palabras en las frases es mayor al parámetro (num_words)"
      ],
      "metadata": {
        "id": "OYj1ClwJg9Ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frases = [\n",
        "'Hola mundo',\n",
        "'Hola a todo',\n",
        "'Hola a todo el mundo',\n",
        "'Buen dia, como estas hoy']\n",
        "#Genera el diccionario de tokens\n",
        "tokenizer = Tokenizer(num_words = 10)\n",
        "tokenizer. fit_on_texts(frases)\n",
        "word_index = tokenizer.word_index\n",
        "print('\\nword_index =',word_index)\n",
        "#Generacion de secuencia tokenizadas\n",
        "secuencias = tokenizer. texts_to_sequences (frases)\n",
        "print('secuencias =',secuencias)\n",
        "#Rellena las secuencias a una longitud uniforme\n",
        "relleno = keras.preprocessing. sequence. pad_sequences (secuencias)\n",
        "print('rellena =\\n',relleno)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRkXy3lKgGcR",
        "outputId": "ca5509db-b04a-4197-eb5f-1d5019e10afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "word_index = {'hola': 1, 'mundo': 2, 'a': 3, 'todo': 4, 'el': 5, 'buen': 6, 'dia': 7, 'como': 8, 'estas': 9, 'hoy': 10}\n",
            "secuencias = [[1, 2], [1, 3, 4], [1, 3, 4, 5, 2], [6, 7, 8, 9]]\n",
            "rellena =\n",
            " [[0 0 0 1 2]\n",
            " [0 0 1 3 4]\n",
            " [1 3 4 5 2]\n",
            " [0 6 7 8 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Número de palabras en las frases es mayor al parametro (num_words) además incluimos el parametro OOV"
      ],
      "metadata": {
        "id": "mx9RsybZg-02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frases = [\n",
        "'Hola mundo',\n",
        "'Hola a todo',\n",
        "'Hola a todo el mundo',\n",
        "'Buen dia, como estas hoy']\n",
        "#Genera el diccionario de tokens\n",
        "tokenizer = Tokenizer(num_words = 10,oov_token=\"<OOV>\")\n",
        "tokenizer. fit_on_texts(frases)\n",
        "word_index = tokenizer.word_index\n",
        "print('\\nword_index =',word_index)\n",
        "#Generacion de secuencia tokenizadas\n",
        "secuencias = tokenizer. texts_to_sequences (frases)\n",
        "print('secuencias =',secuencias)\n",
        "#Rellena las secuencias a una longitud uniforme\n",
        "relleno = keras.preprocessing. sequence. pad_sequences (secuencias)\n",
        "print('rellena =\\n',relleno)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpqjy3WygX8f",
        "outputId": "832c1f44-275a-4fcb-c017-74cd9c4edb9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "word_index = {'<OOV>': 1, 'hola': 2, 'mundo': 3, 'a': 4, 'todo': 5, 'el': 6, 'buen': 7, 'dia': 8, 'como': 9, 'estas': 10, 'hoy': 11}\n",
            "secuencias = [[2, 3], [2, 4, 5], [2, 4, 5, 6, 3], [7, 8, 9, 1, 1]]\n",
            "rellena =\n",
            " [[0 0 0 2 3]\n",
            " [0 0 2 4 5]\n",
            " [2 4 5 6 3]\n",
            " [7 8 9 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Parámetros padding y truncating de la función pad_sequences en\n",
        "Keras se utilizan para controlar el relleno y el truncamiento de las secuencias durante el preprocesamiento de datos."
      ],
      "metadata": {
        "id": "aMiyObWqhgzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frases = [\n",
        "'Hola mundo',\n",
        "'Hola a todo',\n",
        "'Hola a todo el mundo',\n",
        "'Buen dia, como estas hoy']\n",
        "#Genera el diccionario de tokens\n",
        "tokenizer = Tokenizer(num_words = 10,oov_token=\"<OOV>\")\n",
        "tokenizer. fit_on_texts(frases)\n",
        "word_index = tokenizer.word_index\n",
        "print('\\nword_index =',word_index)\n",
        "#Generacion de secuencia tokenizadas\n",
        "secuencias = tokenizer. texts_to_sequences (frases)\n",
        "print('secuencias =',secuencias)\n",
        "#Rellena las secuencias a una longitud uniforme\n",
        "relleno = keras.preprocessing. sequence. pad_sequences (secuencias,padding = 'post', truncating = 'post')\n",
        "print('rellena =\\n',relleno)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMRKcseShjuM",
        "outputId": "790564a0-4f72-4d47-b476-9e65b359231e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "word_index = {'<OOV>': 1, 'hola': 2, 'mundo': 3, 'a': 4, 'todo': 5, 'el': 6, 'buen': 7, 'dia': 8, 'como': 9, 'estas': 10, 'hoy': 11}\n",
            "secuencias = [[2, 3], [2, 4, 5], [2, 4, 5, 6, 3], [7, 8, 9, 1, 1]]\n",
            "rellena =\n",
            " [[2 3 0 0 0]\n",
            " [2 4 5 0 0]\n",
            " [2 4 5 6 3]\n",
            " [7 8 9 1 1]]\n"
          ]
        }
      ]
    }
  ]
}