{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "daYPNd6j823T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nse2k5Jw60xs",
        "outputId": "9d85e845-f6ea-475b-df80-1a064e96c34a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acción seleccionada por el agente: abajo\n"
          ]
        }
      ],
      "source": [
        "class AgenteRL:\n",
        "  def __init__(self, acciones):\n",
        "    self.acciones = acciones\n",
        "\n",
        "  def seleccionar_accion(self, estado):\n",
        "  # Ejemplo de seleccién aleatoria de acción\n",
        "    return random.choice(self.acciones)\n",
        "\n",
        "# Uso del agente RL\n",
        "acciones_posibles = ['izquierda', 'derecha', 'arriba', 'abajo']\n",
        "agente = AgenteRL(acciones_posibles)\n",
        "estado_actual = [0, 0] # Estado inicial del entorno\n",
        "accion_seleccionada = agente.seleccionar_accion(estado_actual)\n",
        "print(\"Acción seleccionada por el agente:\", accion_seleccionada)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EntornoRL:\n",
        "  def __init__(self, estados):\n",
        "    self.estados = estados\n",
        "\n",
        "  def tomar_accion(self, accion):\n",
        "\n",
        "  # Simulación de la transición de estado\n",
        "    nuevo_estado = random.choice(self.estados)\n",
        "    recompensa = random.randint(-10, 10)\n",
        "\n",
        "    return nuevo_estado, recompensa\n",
        "\n",
        "# Uso del entorno RL\n",
        "estados_posibles = ['A', 'B', 'C', 'D']\n",
        "entorno = EntornoRL(estados_posibles)\n",
        "accion = 'izquierda' # Acción seleccionada por el agente\n",
        "nuevo_estado, recompensa = entorno.tomar_accion(accion)\n",
        "print(\"Nuevo estado:\", nuevo_estado)\n",
        "print(\"Recompensa recibida:\", recompensa)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Del9wopu8It-",
        "outputId": "5de850d2-70f6-4552-e106-730d114f1f49"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nuevo estado: D\n",
            "Recompensa recibida: -9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearning:\n",
        "    def __init__(self, estados, acciones, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "        self.estados = estados\n",
        "        self.acciones = acciones\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.q_table = {}\n",
        "\n",
        "    def actualizar_q_table(self, estado_actual, accion, recompensa, nuevo_estado):\n",
        "        if (estado_actual) not in self.q_table:\n",
        "            self.q_table[(estado_actual)] = {a:0 for a in self.acciones}\n",
        "        if (nuevo_estado) not in self.q_table:\n",
        "            self.q_table[(nuevo_estado)] = {a:0 for a in self.acciones}\n",
        "\n",
        "        q_actual = self.q_table[(estado_actual)][accion]\n",
        "        max_q_nuevo_estado = max(self.q_table[(nuevo_estado)].values())\n",
        "        nuevo_q_valor = q_actual + self.alpha * (recompensa + self.gamma * max_q_nuevo_estado - q_actual)\n",
        "        self.q_table[(estado_actual)][accion] = nuevo_q_valor"
      ],
      "metadata": {
        "id": "yBKMQVGy9Ur2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uso del algoritmo Q-Learning\n",
        "estados = ['A', 'B', 'C']\n",
        "acciones = ['izquierda', 'derecha']\n",
        "q_learning = QLearning(estados, acciones)\n",
        "\n",
        "# Simulación de una época de entrenamiento\n",
        "estado_actual = 'A'\n",
        "accion = 'izquierda'\n",
        "nuevo_estado = 'B'\n",
        "recompensa = 10\n",
        "q_learning.actualizar_q_table(estado_actual, accion, recompensa, nuevo_estado)\n",
        "\n",
        "# Visualización de la tabla Q\n",
        "print(\"Tabla Q actualizada:\")\n",
        "print(q_learning.q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD-osADt_6Dm",
        "outputId": "d616090f-150c-4369-e11e-b59d0e1709c1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabla Q actualizada:\n",
            "{'A': {'izquierda': 1.0, 'derecha': 0}, 'B': {'izquierda': 0, 'derecha': 0}}\n"
          ]
        }
      ]
    }
  ]
}