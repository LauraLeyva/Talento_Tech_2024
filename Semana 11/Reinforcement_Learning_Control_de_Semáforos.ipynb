{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "Ztj_dOC4FuWu"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JHrNlXiFn6x",
        "outputId": "9f8e8838-219d-45b0-a994-9841774f17ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabla Q después del entrenamiento:\n",
            "[[-0.11874013 -0.1       ]\n",
            " [ 1.         -1.08455467]\n",
            " [-0.45002694  0.99582544]\n",
            " [ 0.          0.        ]]\n"
          ]
        }
      ],
      "source": [
        "# Definición del entorno\n",
        "n_states = 4  # Número de estados posibles (2 estados por semáforo, por lo que 2*2=4)\n",
        "n_actions = 2  # Acciones posibles (cambiar estado de semáforos A o B)\n",
        "\n",
        "# Definición de la tabla Q\n",
        "Q = np.zeros((n_states, n_actions))\n",
        "\n",
        "# Parámetros de Q-Learning\n",
        "gamma = 0.9  # Factor de descuento\n",
        "alpha = 0.1  # Tasa de aprendizaje\n",
        "epsilon = 0.1  # Tasa de exploración\n",
        "num_episodes = 1000\n",
        "\n",
        "def get_next_state(state, action):\n",
        "    semaforo_A, semaforo_B = state\n",
        "\n",
        "    if action == 0:  # Acción: Cambiar el semáforo A\n",
        "        semaforo_A = 1 - semaforo_A\n",
        "    else:  # Acción: Cambiar el semáforo B\n",
        "        semaforo_B = 1 - semaforo_B\n",
        "\n",
        "    return (semaforo_A, semaforo_B)\n",
        "\n",
        "def get_reward(state):\n",
        "    semaforo_A, semaforo_B = state\n",
        "    if semaforo_A == 1 and semaforo_B == 1:  # Ambos en verde\n",
        "        return 1\n",
        "    elif semaforo_A == 0 or semaforo_B == 0:  # Alguno en rojo\n",
        "        return -1\n",
        "    else:  # Estados intermedios\n",
        "        return 0\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = (0, 0)  # Estado inicial (ambos semáforos en rojo)\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Selección de acción basada en la política epsilon-greedy\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = random.randint(0, n_actions - 1)\n",
        "        else:\n",
        "            action = np.argmax(Q[state[0] * 2 + state[1], :])\n",
        "\n",
        "        next_state = get_next_state(state, action)\n",
        "        reward = get_reward(next_state)\n",
        "\n",
        "        # Actualización de la tabla Q\n",
        "        next_max = np.max(Q[next_state[0] * 2 + next_state[1], :])\n",
        "        Q[state[0] * 2 + state[1], action] = (1 - alpha) * Q[state[0] * 2 + state[1], action] + alpha * (reward + gamma * next_max)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        # Terminamos el episodio si llegamos al estado objetivo (opcional)\n",
        "        if state == (1, 1):\n",
        "            done = True\n",
        "\n",
        "print(\"Tabla Q después del entrenamiento:\")\n",
        "print(Q)"
      ]
    }
  ]
}