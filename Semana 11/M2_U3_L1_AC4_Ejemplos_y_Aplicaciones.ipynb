{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "RUkhpne-8dIy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 1: Implementación de Q-Learning en un entorno de gridworld simple"
      ],
      "metadata": {
        "id": "lhwsrtJAAaVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definición del gridworld\n",
        "gridworld = np.array([\n",
        "    [-1, -1, -1, 1],\n",
        "    [-1, -1, -1, -1],\n",
        "    [-1, -1, -1, -1],\n",
        "    [-1, -1, -1, -1],\n",
        "])\n",
        "\n",
        "# Definición de las acciones posibles: arriba, abajo, izquierda, derecha\n",
        "acciones = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
        "num_acciones = len(acciones)\n",
        "\n",
        "# Implementación de Q-Learning\n",
        "# La forma de Q ahora es (filas, columnas, num_acciones)\n",
        "Q = np.zeros((gridworld.shape[0], gridworld.shape[1], num_acciones))\n",
        "\n",
        "gamma = 0.8\n",
        "alpha = 0.1\n",
        "num_episodios = 1000\n",
        "\n",
        "for _ in range(num_episodios):\n",
        "    estado = (0, 0)\n",
        "    while estado != (0, 3):\n",
        "        # Selecciona una acción basada en la política epsilon-greedy, aquí elegimos aleatoriamente\n",
        "        accion_idx = np.random.choice(range(num_acciones))\n",
        "        accion = acciones[accion_idx]\n",
        "        nueva_fila = estado[0] + accion[0]\n",
        "        nueva_col = estado[1] + accion[1]\n",
        "\n",
        "        # Verifica si el nuevo estado es válido\n",
        "        if 0 <= nueva_fila < gridworld.shape[0] and 0 <= nueva_col < gridworld.shape[1]:\n",
        "            recompensa = gridworld[nueva_fila, nueva_col]\n",
        "            # Calcular el nuevo valor Q\n",
        "            nuevo_valor = recompensa + gamma * np.max(Q[nueva_fila, nueva_col])\n",
        "            # Actualiza el valor Q para el estado y acción actuales\n",
        "            Q[estado[0], estado[1], accion_idx] = (1 - alpha) * Q[estado[0], estado[1], accion_idx] + alpha * nuevo_valor\n",
        "            # Actualiza el estado\n",
        "            estado = (nueva_fila, nueva_col)\n",
        "\n",
        "print(\"Tabla Q-values después del entrenamiento:\")\n",
        "print(Q)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjMz9YJiA5LI",
        "outputId": "bae94b4f-4eed-49a9-c8cf-a583826a1205"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabla Q-values después del entrenamiento:\n",
            "[[[ 0.   -1.    0.   -1.  ]\n",
            "  [ 0.   -1.8  -1.   -0.2 ]\n",
            "  [ 0.   -1.16 -1.    1.  ]\n",
            "  [ 0.    0.    0.    0.  ]]\n",
            "\n",
            " [[-1.   -1.    0.   -1.8 ]\n",
            "  [-1.   -1.8  -1.   -1.16]\n",
            "  [-0.2  -1.8  -1.8  -0.2 ]\n",
            "  [ 1.   -1.   -1.16  0.  ]]\n",
            "\n",
            " [[-1.   -1.    0.   -1.8 ]\n",
            "  [-1.8  -1.   -1.   -1.8 ]\n",
            "  [-1.16 -1.   -1.8  -1.  ]\n",
            "  [-0.2  -1.   -1.8   0.  ]]\n",
            "\n",
            " [[-1.    0.    0.   -1.  ]\n",
            "  [-1.8   0.   -1.   -1.  ]\n",
            "  [-1.8   0.   -1.   -1.  ]\n",
            "  [-1.    0.   -1.    0.  ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 2: Aplicación del Aprendizaje por refuerzo en juegos"
      ],
      "metadata": {
        "id": "9v9fML1nCWcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo: Implementación de Q-Learning para juego simple\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#Definición de las recompensas del juego (datos ficticios)\n",
        "recompensa = {\n",
        "    'ganar': 1,\n",
        "    'perder': -1,\n",
        "    'empatar': 0,\n",
        "}\n",
        "\n",
        "#Implementación de Q-Learning para el juego\n",
        "Q={}\n",
        "\n",
        "def q_learning(estado_actual, accion, nuevo_estado, resultado):\n",
        "    if (estado_actual, accion) not in Q:\n",
        "        Q[(estado_actual, accion)] = np.zeros(len(acciones))\n",
        "    if nuevo_estado not in Q:\n",
        "       Q[nuevo_estado] = np.zeros(len(acciones))\n",
        "       nuevo_valor = recompensa[resultado] + gamma * np.max(Q[nuevo_estado])\n",
        "       Q[(estado_actual, accion)] = (1-alpha) * Q[(estado_actual, accion)] + alpha * nuevo_valor\n",
        "\n",
        "# Ejemplo de variables de entrada (deberían ser determinadas por la lógica del juego)\n",
        "estado_actual = 'estado_inicial'\n",
        "accion = 'abajo'\n",
        "nuevo_estado = 'estado_siguiente'\n",
        "resultado = 'ganar'  # Este valor podría ser 'ganar', 'perder', o 'empatar'\n",
        "\n",
        "# Llamada a la función Q-Learning con las variables de ejemplo\n",
        "q_learning(estado_actual, accion, nuevo_estado, resultado)\n",
        "\n",
        "# Muestra la tabla Q después del entrenamiento\n",
        "print(\"Tabla Q-values después del entrenamiento:\")\n",
        "print(Q)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1KzAbvo_jxD",
        "outputId": "8a0c9995-e043-4a9f-8d40-346ecd1c1628"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tabla Q-values después del entrenamiento:\n",
            "{('estado_inicial', 'abajo'): array([0.1, 0.1, 0.1, 0.1]), 'estado_siguiente': array([0., 0., 0., 0.])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 3: Aplicación del aprendizaje por refuerzo en robótica"
      ],
      "metadata": {
        "id": "vszwHfVHC2Lg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Definición del entorno de navegación (datos ficticios)\n",
        "entorno = np.array([\n",
        "    [0, 0, 0, 0, 0],\n",
        "    [0, -1, -1, -1, 0],\n",
        "    [0, 0, -1, 0, 0],\n",
        "    [0, -1, -1, -1, 0],\n",
        "    [0, 0, 0, 0, 0]\n",
        "])\n",
        "\n",
        "# Definición de acciones posibles (arriba, abajo, izquierda, derecha)\n",
        "acciones = [(0, -1), (0, 1), (-1, 0), (1, 0)]\n",
        "\n",
        "# Implementación de Q-Learning\n",
        "Q = np.zeros((entorno.shape[0], entorno.shape[1], len(acciones)))\n",
        "\n",
        "gamma = 0.9  # Factor de descuento\n",
        "alpha = 0.1  # Tasa de aprendizaje\n",
        "num_episodes = 1000\n",
        "\n",
        "# Definición del objetivo (estado terminal)\n",
        "estado_objetivo = (4, 4)  # Coordenadas del estado objetivo en el entorno\n",
        "\n",
        "for _ in range(num_episodes):\n",
        "    estado = (0, 0)\n",
        "    while estado != estado_objetivo:\n",
        "        # Selección de acción basada en la política epsilon-greedy (aquí simplemente aleatoria)\n",
        "        accion = np.random.choice(range(len(acciones)))\n",
        "        nueva_fila = estado[0] + acciones[accion][0]\n",
        "        nueva_col = estado[1] + acciones[accion][1]\n",
        "\n",
        "        # Verificación de los límites del entorno\n",
        "        if 0 <= nueva_fila < entorno.shape[0] and 0 <= nueva_col < entorno.shape[1]:\n",
        "            recompensa = entorno[nueva_fila, nueva_col]\n",
        "            # Actualización del valor Q\n",
        "            nuevo_valor = recompensa + gamma * np.max(Q[nueva_fila, nueva_col, :])\n",
        "            Q[estado[0], estado[1], accion] = (1 - alpha) * Q[estado[0], estado[1], accion] + alpha * nuevo_valor\n",
        "            estado = (nueva_fila, nueva_col)\n",
        "\n",
        "print(\"Valores Q después del entrenamiento:\")\n",
        "print(Q)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Blcfac0y_mz3",
        "outputId": "4e417330-ad5a-405f-d803-48541416d2fa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valores Q después del entrenamiento:\n",
            "[[[ 0.  0.  0.  0.]\n",
            "  [ 0.  0.  0. -1.]\n",
            "  [ 0.  0.  0. -1.]\n",
            "  [ 0.  0.  0. -1.]\n",
            "  [ 0.  0.  0.  0.]]\n",
            "\n",
            " [[ 0. -1.  0.  0.]\n",
            "  [ 0. -1.  0.  0.]\n",
            "  [-1. -1.  0. -1.]\n",
            "  [-1.  0.  0.  0.]\n",
            "  [-1.  0.  0.  0.]]\n",
            "\n",
            " [[ 0.  0.  0.  0.]\n",
            "  [ 0. -1. -1. -1.]\n",
            "  [ 0.  0. -1. -1.]\n",
            "  [-1.  0. -1. -1.]\n",
            "  [ 0.  0.  0.  0.]]\n",
            "\n",
            " [[ 0. -1.  0.  0.]\n",
            "  [ 0. -1.  0.  0.]\n",
            "  [-1. -1. -1.  0.]\n",
            "  [-1.  0.  0.  0.]\n",
            "  [-1.  0.  0.  0.]]\n",
            "\n",
            " [[ 0.  0.  0.  0.]\n",
            "  [ 0.  0. -1.  0.]\n",
            "  [ 0.  0. -1.  0.]\n",
            "  [ 0.  0. -1.  0.]\n",
            "  [ 0.  0.  0.  0.]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio 4: Aplicación del aprendizaje por refuerzo en gestión de recursos"
      ],
      "metadata": {
        "id": "SEItYF_PDTlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Definición de los estados (niveles de inventario), acciones (órdenes de reabastecimiento) y recompensar (costos, ganancias, etc.)\n",
        "\n",
        "estados = ['Bajo', 'Medio', 'Alto']\n",
        "acciones = ['Reabastecer', 'No reabastecer']\n",
        "recompensas = {\n",
        "    ('Bajo', 'Reabastecer'): 50,\n",
        "    ('Bajo', 'No reabastecer'): -10,\n",
        "    ('Medio', 'Reabastecer'): 30,\n",
        "    ('Medio', 'No reabastecer'): 0,\n",
        "    ('Alto', 'Reabastecer'): 10,\n",
        "    ('Alto', 'No reabastecer'): -20,\n",
        "}\n",
        "\n",
        "# Implementación de Q-Learning\n",
        "Q = {}\n",
        "\n",
        "gamma = 0.9 #Factor de descuento\n",
        "alpha = 0.1 #Tasa de aprendizaje\n",
        "num_episodes = 1000\n",
        "\n",
        "for _ in range(num_episodes):\n",
        "    estado_actual = np.random.choice(estados)\n",
        "    while True:\n",
        "        accion = np.random.choice(acciones)\n",
        "        recompensa = recompensas[(estado_actual, accion)]\n",
        "\n",
        "        if (estado_actual) not in Q:\n",
        "            Q[estado_actual] = {}\n",
        "        if accion not in Q[estado_actual]:\n",
        "            Q[estado_actual][accion] = 0\n",
        "\n",
        "        nuevo_estado = np.random.choice(estados)\n",
        "        max_nuevo_estado = max(Q[nuevo_estado].values()) if nuevo_estado in Q else 0\n",
        "        Q[estado_actual][accion] += alpha * (recompensa + gamma * max_nuevo_estado - Q[estado_actual][accion])\n",
        "        estado_actual = nuevo_estado\n",
        "        if recompensa == 50 or recompensa ==30 or recompensa == 10:\n",
        "           break\n",
        "\n",
        "print(\"Valores Q después del entrenamiento:\")\n",
        "print(Q)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sNodQpK_o9r",
        "outputId": "f3775653-ebe2-4663-d462-877d171fb97c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valores Q después del entrenamiento:\n",
            "{'Medio': {'No reabastecer': 248.93904353664368, 'Reabastecer': 270.34613001251074}, 'Bajo': {'No reabastecer': 241.1434589637322, 'Reabastecer': 299.1470531783196}, 'Alto': {'No reabastecer': 229.79178875263509, 'Reabastecer': 260.3299471728231}}\n"
          ]
        }
      ]
    }
  ]
}