{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ejercicio de Aprendizaje por Refuerzo en Python"
      ],
      "metadata": {
        "id": "TSwmCVxY8eNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "RUkhpne-8dIy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejercicio 1: Introducción a los principales algoritmos de RL\n",
        "# Define el entorno del juego\n",
        "\n",
        "class Environment:\n",
        "    def __init__(self):\n",
        "        self.state_space = np.array([0, 1, 2, 3]) #Estados posibles\n",
        "        self.action_space = np.array([0, 1]) #Acciones posibles\n",
        "        self.rewards = {0: -1, 1: -1, 2: -1, 3: 10} #Recompensas por estado\n",
        "\n",
        "#Crea una instancia del entorno\n",
        "env = Environment()\n",
        "\n",
        "#Nuestra información del entorno\n",
        "print(\"Estados posibles:\", env.state_space)\n",
        "print(\"Acciones posibles:\", env.action_space)\n",
        "print(\"Recompensas por estado:\", env.rewards)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjMz9YJiA5LI",
        "outputId": "ab88539d-084b-412c-bb4b-62fc4c90988f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estados posibles: [0 1 2 3]\n",
            "Acciones posibles: [0 1]\n",
            "Recompensas por estado: {0: -1, 1: -1, 2: -1, 3: 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejercicio: Q-Learning\n",
        "import numpy as np\n",
        "\n",
        "# Inicializa la tabla Q con valores arbitrarios\n",
        "Q = np.zeros((len(env.state_space), len(env.action_space)))\n",
        "\n",
        "#Define los parámetros del algortimo\n",
        "alpha = 0.1 #Tasa de aprendizaje\n",
        "gamma = 0.9 #Factor de descuento\n",
        "\n",
        "# Entrena el agente utilizando Q-Learning\n",
        "for _ in range(1000):\n",
        "    state = np.random.choice(env.state_space)  # Estado inicial aleatorio\n",
        "    while state != 3:  # Hasta llegar al estado objetivo\n",
        "        action = np.random.choice(env.action_space)  # Selecciona una acción aleatoria\n",
        "        next_state = state + action\n",
        "        reward = env.rewards[next_state]\n",
        "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]) - Q[state, action])\n",
        "        state = next_state\n",
        "\n",
        "#Imprime la tabla Q final\n",
        "print(\"Función Q-valor aprendida:\")\n",
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZtbzEK6HUA5",
        "outputId": "adb38e64-5374-4e40-a76a-e26859dad312"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Función Q-valor aprendida:\n",
            "[[ 4.58  6.2 ]\n",
            " [ 6.2   8.  ]\n",
            " [ 8.   10.  ]\n",
            " [ 0.    0.  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ejercicio 3: Sarsa\n",
        "#Reinicializa la tabla Q con valore arbitrarios\n",
        "Q = np.zeros((len(env.state_space), len(env.action_space)))\n",
        "\n",
        "#Entrena el agente utilizando Sarsa\n",
        "for _ in range(1000):\n",
        "    state = np.random.choice(env.state_space) #Estado inicial\n",
        "    action = np.random.choice(env.action_space) #Selecciona una acción aleatoria\n",
        "    while state != 3: #Hasta llegar al estado objetivo\n",
        "        next_state = state + action\n",
        "        next_action = np.random.choice(env.action_space) #Selecciona una acción aleatoria\n",
        "        reward = env.rewards[next_state] #Recompensa por el estado siguiente\n",
        "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[next_state, next_action] - Q[state, action])\n",
        "        state = next_state #Actualiza el estado\n",
        "        action = next_action #Actualiza la acción\n",
        "\n",
        "#Muestre la función Q-valor que aprendió con Sarsa\n",
        "print(\"Función Q-valor aprendida con Sarsa:\")\n",
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5nvq13zHUov",
        "outputId": "86e6d85f-65d8-4c0e-d661-dcb6fe10de8a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Función Q-valor aprendida con Sarsa:\n",
            "[[ 1.42128041  3.58103933]\n",
            " [ 3.68866315  6.3370109 ]\n",
            " [ 7.1567114  10.        ]\n",
            " [ 0.          0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejercicio 4: Política de Gradiente de Montecarlo\n",
        "# Inicializa la política con probabilidades uniformes\n",
        "\n",
        "# Supongamos que env.state_space y env.action_space ya están definidos.\n",
        "# Reemplaza los valores ficticios con los valores reales de tu entorno.\n",
        "\n",
        "# Inicializa la política con probabilidades uniformes\n",
        "policy = np.ones((len(env.state_space), len(env.action_space))) / len(env.action_space)\n",
        "\n",
        "# Define la función de recompensa promedio\n",
        "def average_reward(Q):\n",
        "    return np.mean([Q[state, np.argmax(policy[state])] for state in env.state_space])\n",
        "\n",
        "# Entrena la política utilizando Gradiente de Montecarlo\n",
        "for _ in range(1000):\n",
        "    state = np.random.choice(env.state_space)  # Estado inicial aleatorio\n",
        "    while state != 3:  # Hasta el estado objetivo\n",
        "        action = np.random.choice(env.action_space, p=policy[state])  # Selecciona una acción basada en la política\n",
        "        next_state = state + action  # Estado siguiente (ajustar según el entorno real)\n",
        "        reward = env.rewards[next_state]  # Recompensa por el estado siguiente\n",
        "\n",
        "        # Calcula el gradiente\n",
        "        gradient = np.zeros_like(policy[state])\n",
        "        gradient[action] = 1\n",
        "\n",
        "        # Tasa de aprendizaje\n",
        "        alpha = 0.01\n",
        "\n",
        "        # Actualiza la política\n",
        "        policy[state] += alpha * gradient * (reward - average_reward(Q))\n",
        "\n",
        "        # Normaliza la política para que las probabilidades sumen 1\n",
        "        policy[state] = np.exp(policy[state])  # Utiliza la función exponencial para asegurar valores positivos\n",
        "        policy[state] /= np.sum(policy[state])  # Normaliza para que sumen 1\n",
        "\n",
        "        state = next_state  # Actualiza el estado\n",
        "\n",
        "# Muestra la política aprendida\n",
        "print(\"Política aprendida por Gradiente de Montecarlo:\")\n",
        "print(policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50jTRvw2HZw5",
        "outputId": "e07c1914-4fbd-45cb-f461-01a480ddac97"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Política aprendida por Gradiente de Montecarlo:\n",
            "[[0.5203344  0.4796656 ]\n",
            " [0.50396567 0.49603433]\n",
            " [0.47036484 0.52963516]\n",
            " [0.5        0.5       ]]\n"
          ]
        }
      ]
    }
  ]
}